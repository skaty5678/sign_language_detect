{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce02637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.4.1\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl (173.9 MB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-macosx_10_15_x86_64.whl (46.3 MB)\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.8.10-cp38-cp38-macosx_10_15_x86_64.whl (33.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Using cached matplotlib-3.5.2-cp38-cp38-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.20.1-cp38-cp38-macosx_10_9_x86_64.whl (962 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-macosx_10_9_x86_64.whl (3.0 MB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Using cached attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.5.5.64-cp36-abi3-macosx_10_15_x86_64.whl (55.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.0-cp38-cp38-macosx_10_13_x86_64.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.2-cp38-cp38-macosx_10_9_x86_64.whl (65 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./action/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./action/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./action/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.1.1-cp38-cp38-macosx_10_10_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./action/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (49.2.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.8.0-cp38-cp38-macosx_12_0_universal2.macosx_10_9_x86_64.whl (55.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for wrapt, since package 'wheel' is not installed.\n",
      "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard-plugin-wit, pyasn1, flatbuffers, certifi, zipp, wheel, werkzeug, urllib3, threadpoolctl, tensorboard-data-server, six, rsa, pyasn1-modules, protobuf, pillow, oauthlib, numpy, kiwisolver, joblib, idna, gast, fonttools, cycler, charset-normalizer, cachetools, attrs, scipy, requests, opt-einsum, opencv-python, opencv-contrib-python, keras-preprocessing, importlib-metadata, h5py, grpcio, google-pasta, google-auth, astunparse, absl-py, scikit-learn, requests-oauthlib, matplotlib, markdown, sklearn, mediapipe, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Running setup.py install for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed absl-py-0.15.0 astunparse-1.6.3 attrs-21.4.0 cachetools-5.1.0 certifi-2021.10.8 charset-normalizer-2.0.12 cycler-0.11.0 flatbuffers-1.12 fonttools-4.33.3 gast-0.3.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 idna-3.3 importlib-metadata-4.11.3 joblib-1.1.0 keras-preprocessing-1.1.2 kiwisolver-1.4.2 markdown-3.3.7 matplotlib-3.5.2 mediapipe-0.8.10 numpy-1.19.5 oauthlib-3.2.0 opencv-contrib-python-4.5.5.64 opencv-python-4.5.5.64 opt-einsum-3.3.0 pillow-9.1.1 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 scikit-learn-1.1.0 scipy-1.8.0 six-1.15.0 sklearn-0.0 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 threadpoolctl-3.1.0 typing-extensions-3.7.4.3 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.12.1 zipp-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d154b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca1ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic #Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils #drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ccf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)#color conversion bgr t0 rgb\n",
    "    image.flags.writeable = False #image is no longer writeable\n",
    "    results = model.process(image) #make prediction\n",
    "    image.flags.writeable = True # image is now writeable\n",
    "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #color conversion rgb to bgr\n",
    "    return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5bd5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)#draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)#draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)#draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)#draw right hand connections\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fa9dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image,results):\n",
    "    #draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             )\n",
    "    \n",
    "    #draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2)\n",
    "                             )\n",
    "      \n",
    "    #draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76),thickness=2,circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2)\n",
    "                             )\n",
    "    \n",
    "    #draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)\n",
    "                             )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f8b0007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "#access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        #read feed\n",
    "        ret,frame = cap.read()\n",
    "        \n",
    "        #make predictions\n",
    "        image,results = mediapipe_detection(frame,holistic)\n",
    "        print(results)\n",
    "        \n",
    "        #draw landmarks\n",
    "        draw_styled_landmarks(image,results)\n",
    "        \n",
    "        #show to screen\n",
    "        cv2.imshow('action',image)\n",
    "        \n",
    "        #break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6e3ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "204682d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x,res.y,res.z,res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40bd3a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.53245223,  0.38332969, -1.07678962,  0.99994761]),\n",
       " array([ 0.55267918,  0.28563944, -1.019135  ,  0.99988002]),\n",
       " array([ 0.56783628,  0.28607142, -1.01911175,  0.99989522]),\n",
       " array([ 0.58242494,  0.28695202, -1.01906931,  0.99986202]),\n",
       " array([ 0.50174582,  0.29142034, -1.0267086 ,  0.99989241]),\n",
       " array([ 0.484584  ,  0.29583085, -1.02598453,  0.9999097 ]),\n",
       " array([ 0.46882996,  0.3007583 , -1.02631021,  0.99989873]),\n",
       " array([ 0.59710294,  0.32830483, -0.62401021,  0.99982846]),\n",
       " array([ 0.44133291,  0.34301579, -0.65979373,  0.99993759]),\n",
       " array([ 0.56423169,  0.46846026, -0.92720366,  0.99993253]),\n",
       " array([ 0.49878231,  0.47251019, -0.93572122,  0.99995971]),\n",
       " array([ 0.7475    ,  0.77418864, -0.37390822,  0.9987945 ]),\n",
       " array([ 0.29689646,  0.80207628, -0.43309587,  0.99928147]),\n",
       " array([ 0.82580853,  1.34506249, -0.47346243,  0.14370134]),\n",
       " array([ 0.18630545,  1.37602043, -0.47264075,  0.38196054]),\n",
       " array([ 0.82824212,  1.57760727, -1.01588535,  0.05760959]),\n",
       " array([ 0.21154039,  1.73094809, -1.12488818,  0.1253714 ]),\n",
       " array([ 0.85357821,  1.66231132, -1.17038989,  0.07073382]),\n",
       " array([ 0.19763364,  1.86648107, -1.27952468,  0.12970231]),\n",
       " array([ 0.81989843,  1.64083982, -1.21191585,  0.10844368]),\n",
       " array([ 0.24203923,  1.83267987, -1.35870814,  0.19422898]),\n",
       " array([ 0.79996598,  1.60526764, -1.0596149 ,  0.10758505]),\n",
       " array([ 0.25513956,  1.7766006 , -1.17890549,  0.19242391]),\n",
       " array([ 6.74120724e-01,  1.77822673e+00, -4.55297157e-02,  1.02920586e-03]),\n",
       " array([3.75702083e-01, 1.78425682e+00, 5.10040820e-02, 1.00000855e-03]),\n",
       " array([6.53169751e-01, 2.60261989e+00, 1.05280757e-01, 4.10707697e-04]),\n",
       " array([3.67020577e-01, 2.61227393e+00, 1.99663803e-01, 9.76034498e-05]),\n",
       " array([6.44786954e-01, 3.35207510e+00, 8.18589568e-01, 4.68032995e-05]),\n",
       " array([3.63159955e-01, 3.35948181e+00, 7.60700405e-01, 4.32982142e-06]),\n",
       " array([6.51391983e-01, 3.48018146e+00, 8.57275188e-01, 4.57926908e-05]),\n",
       " array([3.58560503e-01, 3.48862314e+00, 7.99059749e-01, 1.06095094e-05]),\n",
       " array([5.92224002e-01, 3.59674501e+00, 2.73644418e-01, 4.95593777e-05]),\n",
       " array([4.04435575e-01, 3.58525825e+00, 1.50491670e-01, 2.74592730e-05])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49efde08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b13dea8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81580305,  0.55668074, -0.03203377, ...,  0.90449858,\n",
       "        0.44078952,  0.02715585])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f605017b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.32452226e-01,  3.83329690e-01, -1.07678962e+00,  9.99947608e-01,\n",
       "        5.52679181e-01,  2.85639435e-01, -1.01913500e+00,  9.99880016e-01,\n",
       "        5.67836285e-01,  2.86071420e-01, -1.01911175e+00,  9.99895215e-01,\n",
       "        5.82424939e-01,  2.86952019e-01, -1.01906931e+00,  9.99862015e-01,\n",
       "        5.01745820e-01,  2.91420341e-01, -1.02670860e+00,  9.99892414e-01,\n",
       "        4.84584004e-01,  2.95830846e-01, -1.02598453e+00,  9.99909699e-01,\n",
       "        4.68829960e-01,  3.00758302e-01, -1.02631021e+00,  9.99898732e-01,\n",
       "        5.97102940e-01,  3.28304827e-01, -6.24010205e-01,  9.99828458e-01,\n",
       "        4.41332906e-01,  3.43015790e-01, -6.59793735e-01,  9.99937594e-01,\n",
       "        5.64231694e-01,  4.68460262e-01, -9.27203655e-01,  9.99932528e-01,\n",
       "        4.98782307e-01,  4.72510189e-01, -9.35721219e-01,  9.99959707e-01,\n",
       "        7.47500002e-01,  7.74188638e-01, -3.73908222e-01,  9.98794496e-01,\n",
       "        2.96896458e-01,  8.02076280e-01, -4.33095872e-01,  9.99281466e-01,\n",
       "        8.25808525e-01,  1.34506249e+00, -4.73462433e-01,  1.43701345e-01,\n",
       "        1.86305448e-01,  1.37602043e+00, -4.72640753e-01,  3.81960541e-01,\n",
       "        8.28242123e-01,  1.57760727e+00, -1.01588535e+00,  5.76095879e-02,\n",
       "        2.11540386e-01,  1.73094809e+00, -1.12488818e+00,  1.25371397e-01,\n",
       "        8.53578210e-01,  1.66231132e+00, -1.17038989e+00,  7.07338229e-02,\n",
       "        1.97633639e-01,  1.86648107e+00, -1.27952468e+00,  1.29702315e-01,\n",
       "        8.19898427e-01,  1.64083982e+00, -1.21191585e+00,  1.08443685e-01,\n",
       "        2.42039233e-01,  1.83267987e+00, -1.35870814e+00,  1.94228977e-01,\n",
       "        7.99965978e-01,  1.60526764e+00, -1.05961490e+00,  1.07585050e-01,\n",
       "        2.55139560e-01,  1.77660060e+00, -1.17890549e+00,  1.92423910e-01,\n",
       "        6.74120724e-01,  1.77822673e+00, -4.55297157e-02,  1.02920586e-03,\n",
       "        3.75702083e-01,  1.78425682e+00,  5.10040820e-02,  1.00000855e-03,\n",
       "        6.53169751e-01,  2.60261989e+00,  1.05280757e-01,  4.10707697e-04,\n",
       "        3.67020577e-01,  2.61227393e+00,  1.99663803e-01,  9.76034498e-05,\n",
       "        6.44786954e-01,  3.35207510e+00,  8.18589568e-01,  4.68032995e-05,\n",
       "        3.63159955e-01,  3.35948181e+00,  7.60700405e-01,  4.32982142e-06,\n",
       "        6.51391983e-01,  3.48018146e+00,  8.57275188e-01,  4.57926908e-05,\n",
       "        3.58560503e-01,  3.48862314e+00,  7.99059749e-01,  1.06095094e-05,\n",
       "        5.92224002e-01,  3.59674501e+00,  2.73644418e-01,  4.95593777e-05,\n",
       "        4.04435575e-01,  3.58525825e+00,  1.50491670e-01,  2.74592730e-05])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50d5abd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de89cc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.34652007e-01,  7.22001851e-01,  7.51291282e-07,  4.04126287e-01,\n",
       "        6.91115975e-01, -3.41656879e-02,  4.62586701e-01,  6.12973273e-01,\n",
       "       -5.11310026e-02,  5.06947160e-01,  5.38657129e-01, -6.58589080e-02,\n",
       "        5.54289937e-01,  5.19796014e-01, -8.02765191e-02,  4.30197567e-01,\n",
       "        4.17138636e-01, -1.93413682e-02,  4.57265973e-01,  3.07686120e-01,\n",
       "       -3.52763832e-02,  4.71285194e-01,  2.36105323e-01, -5.05069196e-02,\n",
       "        4.82401073e-01,  1.77362949e-01, -6.27013147e-02,  3.87476206e-01,\n",
       "        3.91702741e-01, -1.88357923e-02,  4.02058542e-01,  2.62379229e-01,\n",
       "       -3.25463898e-02,  4.09028143e-01,  1.78855509e-01, -4.73402664e-02,\n",
       "        4.15636659e-01,  1.05670989e-01, -5.82983494e-02,  3.44162971e-01,\n",
       "        3.99259806e-01, -2.31353827e-02,  3.44710290e-01,  2.74706185e-01,\n",
       "       -4.19679955e-02,  3.47805977e-01,  1.94148928e-01, -5.88929132e-02,\n",
       "        3.52817655e-01,  1.24279708e-01, -6.98900968e-02,  2.97820330e-01,\n",
       "        4.36727166e-01, -3.05621512e-02,  2.71286428e-01,  3.49561840e-01,\n",
       "       -5.06771654e-02,  2.56485879e-01,  2.90576518e-01, -6.19744025e-02,\n",
       "        2.47819066e-01,  2.30733693e-01, -6.85387328e-02])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0d44a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x,res.y,res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,face,lh,rh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf1b89cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1662,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keypoints(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "683f5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for exported data, numpy array\n",
    "DATA_PATH = os.path.join('MP_data')\n",
    "\n",
    "#actions that we try to detect\n",
    "actions = np.array(['hello','thankyou','iloveyou'])\n",
    "\n",
    "#thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "#videos are going to be thirty frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae4fdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3f8e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                #print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db9ae4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d573c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2003227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thankyou': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97b7b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1692ab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 30, 1662)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd24440f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f3af56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "619f2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0aa989f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d3b2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0759a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a1c93bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 04:01:27.076811: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2022-05-18 04:01:27.076849: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2022-05-18 04:01:27.076924: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e65c5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 04:02:33.884284: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-18 04:02:33.884724: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e63592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam',loss = 'categorical_Crossentropy',metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72c045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e4c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "action",
   "language": "python",
   "name": "action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
